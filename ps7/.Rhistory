library(tidyverse)
library(tidyverse)
set.seed(89)
genX <- function(n) {
return(
data.frame(X1 = runif(n,0,1),
X2 = runif(n,0,1),
X3 = runif(n,0,1),
X4 = runif(n,0,1),
X5 = runif(n,0,1))
)
}
genY <- function(X) {
Ylin <- 10*I(X$X1>0.7)-20*I(X$X2 < 0.35)
Yp <- 1/(1+exp(-Ylin))
Y <- rbinom(nrow(X),1,Yp)
return(Y)
}
# Generate 1000 observations and corresponding labels
train_X <- genX(1000)
train_y <- genY(train_X)
ggplot(data = cbind(train_X, y = train_y),
aes(x = X1, y = X2, color = as.factor(y))) +
geom_point(size = 2) +
theme_bw() +
labs(color = "Class") +
theme(legend.position = "bottom")
1/(1+exp(-10))
1/(1+exp(-20))
rbinom(10,1,1/(1+exp(-10)))
20*I(0.2 < 0.35)
1/(1+exp(10))
# calculate the node purity using the gini index
gini_index <- function(y) {
# calculate each class component of the sum
gini_c <- sapply(unique(y), function (a) mean(y == a)*(1-mean(y == a)))
return(sum(gini_c))
}
# calculate the gain in node purity given a split
information_gain <- function(y_parent, l_idx, r_idx, criterion = gini_index) {
y_left <- y_parent[l_idx]
y_right <- y_parent[r_idx]
n <- length(y_parent)
n_l <- length(y_left)
n_r <- length(y_right)
I_p <- criterion(y_parent)
I_l <- criterion(y_left)
I_r <- criterion(y_right)
ig <- I_p - ((n_l)/n * I_l + (n_r)/n * I_r)
return(ig)
}
# find best split for single variable
best_var_split <- function(y, predictor, criterion = gini_index) {
split_vals <- sort(unique(predictor))[-1]
igs <- sapply(split_vals, function (c) {
l_idx <- which(predictor < c)
r_idx <- which(predictor >= c) # the remaining indices
return(information_gain(y, l_idx, r_idx, criterion = criterion))
})
return(
list(
ig = max(igs),
c = split_vals[which.max(igs)]
)
)
}
# find best variable
find_split <- function(y, X, criterion = gini_index) {
best_igs <- apply(X, 2, function (x) best_var_split(y, x, criterion = criterion))
best_idx <- which.max(sapply(best_igs, function (x) x$ig))
return(
list(
var = names(X)[best_idx],
c = best_igs[[best_idx]]$c
)
)
}
if (gini_index(train_y) == 0) {
mean(train_y)
} else if (depth == MAX_DEPTH) {
mean(train_y)
}
MAX_DEPTH = 10
depth = 0 # we're at the root of our tree initially
if (gini_index(train_y) == 0) {
mean(train_y)
} else if (depth == MAX_DEPTH) {
mean(train_y)
}
best_split <- best_var_split(train_y, train_X)
View(train_X)
# find best split for single variable
best_var_split <- function(y, predictor, criterion = gini_index) {
split_vals <- sort(unique(predictor))[-1]
igs <- sapply(split_vals, function (c) {
l_idx <- which(predictor < c)
r_idx <- which(predictor >= c) # the remaining indices
return(information_gain(y, l_idx, r_idx, criterion = criterion))
})
return(
list(
ig = max(igs),
c = split_vals[which.max(igs)]
)
)
}
best_split <- best_var_split(train_y, train_X)
best_split <- find_split(train_y, train_X)
best_split
decision <- paste0(best_split$var,"=",best_split$c)
print(decision)
# in the first position of this subtree,
# create an empty list to store our branches
subtree[[decision]] <- list()
# create a subtree object
subtree <- list()
# in the first position of this subtree,
# create an empty list to store our branches
subtree[[decision]] <- list()
View(subtree)
# find the left and right indices
left <- train_X[[best_split$var]] < best_split$c
right <- !left
# store the subsets in our subtree
subtree[[decision]] <- append(subtree[[decision]], list('l' = train_X[left,]))
subtree[[decision]] <- append(subtree[[decision]], list('r' = train_X[right,]))
grow <- function(y, X, criterion = gini_index, depth = 0, MAX_DEPTH = 10) {
# step 1
if (criterion(y) == 0) { # notice we've changed something here!
return(mean(y)) # added a return call (as we're in a function!)
} else if (depth == MAX_DEPTH) {
return(mean(y))
}
# step 2
best_split <- find_split(y,X)
decision <- paste0(best_split$var,"<",best_split$c)
# step 3
subtree <- list()
subtree[[decision]] <- list()
left <- X[[best_split$var]] < best_split$c
right <- !left
# CODE DIFFERS FROM HERE
left_branch <- grow( # <- recursive call
y = y[left],
X = X[left, ],
criterion = criterion,
depth = depth + 1,
MAX_DEPTH = MAX_DEPTH
)
right_branch <- grow( # <- recursive call
y[right],
X[right, ],
criterion = criterion,
depth = depth + 1,
MAX_DEPTH = MAX_DEPTH
)
# when the branches have evaluated, store the resulting subtree
subtree[[decision]] <- append(subtree[[decision]], left_branch)
subtree[[decision]] <- append(subtree[[decision]], right_branch)
class(subtree) <- c("tree", "list") # set class so R knows what type of object
return(subtree)
}
tree <- grow(train_y, train_X)
View(tree)
decision <- paste0(best_split$var,"<",best_split$c)
parse_decision <- function(decision) {
arguments <- strsplit(decision, split = "<")
var <- arguments[[1]][1] # this is just annoying indexing in R, it's the first item of the first list entry
val <- as.numeric(arguments[[1]][2]) # and likewise, this is the second item...
return(list(var = var, val = val))
}
names(tree)
predict.tree <- function(tree, Xtest) {
# let's use apply to loop through every row
preds <- apply(Xtest, 1, function (x) {
pred <- NA # start with no prediction
tree_c <- tree # take a copy of the tree structure
while (is.na(pred)) { # keep moving through tree until we have a prediction
# extract the splitting variable and criteria
decision <- parse_decision(names(tree_c))
# Make decision: 1 = "left" and 2 = "right"
if (x[decision$var] < decision$val) {
dir = 1
} else {
dir = 2
}
# Now we just check whether the resulting split is terminal
if (is.numeric(tree_c[[1]][[dir]])) {
pred = tree_c[[1]][[dir]]
} else {
tree_c = tree_c[[1]][dir] # extract the remaining subtree
}
}
return(pred)
})
return(preds)
}
test_X <- genX(1000)
test_y <- genY(test_X)
yhat_probs <- predict.tree(tree, test_X)
yhat <- round(yhat_probs)
print(yhat[1:10])
# Unconditional accuracy
mean(test_y == yha)
# Unconditional accuracy
mean(test_y == yhat)
# Accuracy for X2 < 0.35 (all red)
c1 <- test_X$X2 < 0.35
mean(test_y[c1] == yhat[c1])
# Accuracy for where X1 > 0.7 (red and blue, but clear decision boundary)
c2 <- test_X$X1 > 0.7
mean(test_y[c2] == yhat[c2])
predict(tree, test_X)
mean(test_y[c2] == yhat[c2])
library(rpart)
library(rpart.plot)
install.packages("rpart.plot")
tree_mod <- rpart(train_y ~ ., train_X, method = "class")
rpart.plot(tree_mod)
library(rpart)
library(rpart.plot)
tree_mod <- rpart(train_y ~ ., train_X, method = "class")
rpart.plot(tree_mod)
pruned_tree <- prune(tree_mod, cp = 0.025)
rpart.plot(pruned_tree)
set.seed(89)
crime <- read.csv("crime.csv")
train_idx <- sample(1:nrow(crime), 0.8*nrow(crime))
crime_train <- crime[train_idx,]
crime_test <- crime[-train_idx,]
y_train <- crime_train$ViolentCrimesPerPop
y_test <- crime_test$ViolentCrimesPerPop
crime_train$ViolentCrimesPerPop <- crime_test$ViolentCrimesPerPop <- 1
cv_loss <- data.frame(
cp = seq(0,0.2,0.01),
CV_error = NA
)
K <- 10
#dirname(rstudioapi::getSourceEditorContext()$path) #get file path
knitr::opts_chunk$set(
collapse = TRUE,
comment = "##",
message = FALSE,
warning = FALSE,
out.extra = "",
root.dir = "D:/OneDrive - London School of Economics/Desktop/lse assignments/MY459/summative-midterm"
)
rm(list=ls())
need <- c('tidyverse',"quanteda","quanteda.textplots","kableExtra","stringi") # list packages needed
have <- need %in% rownames(installed.packages()) # checks packages you have
if(any(!have)) install.packages(need[!have]) # install missing packages
invisible(lapply(need, library, character.only=T))
setwd("D:/OneDrive - London School of Economics/Desktop/lse assignments/MY459/summative-midterm") #change to your own directory when running chunks in R
#download.file('https://github.com/lse-my459/pset_data/raw/master/candidate-tweets.csv', 'candidate-tweets.csv')
tweets <- read.csv('candidate-tweets.csv', stringsAsFactors=F)
with <- grep("I'm with (her|him|them)",tweets$text,ignore.case=TRUE, value=T)
head(with,10)
with_hash <- grep("(I'm with (her|him|them))|#Imwithher",tweets$text,ignore.case=TRUE, value=T)
head(with_hash,10)
make_great <- grep("make .* great again",tweets$text,ignore.case=TRUE, value=T)
head(make_great,10)
make_great_no_america <- str_subset(make_great, regex("make america great again", ignore_case=T), negate=T)
head(make_great_no_america,10)
make_great_no_america
make_great <- grep("make .* great again",tweets$text,ignore.case=TRUE, value=T)
head(make_great,10)
make_great_no_america <- str_subset(make_great, regex("make america great again", ignore_case=T), negate=T)
head(make_great_no_america,10)
library(quanteda.textmodels)
#dirname(rstudioapi::getSourceEditorContext()$path) #get file path
knitr::opts_chunk$set(
collapse = TRUE,
comment = "##",
message = FALSE,
warning = FALSE,
out.extra = "",
root.dir = "D:/OneDrive - London School of Economics/Desktop/lse assignments/MY459/summative-midterm"
)
rm(list=ls())
need <- c('tidyverse',"quanteda","quanteda.textmodels","kableExtra","caret","glmnet","glmnetUtils","parallel") # list packages needed
have <- need %in% rownames(installed.packages()) # checks packages you have
if(any(!have)) install.packages(need[!have]) # install missing packages
invisible(lapply(need, library, character.only=T))
setwd("D:/OneDrive - London School of Economics/Desktop/lse assignments/MY459/summative-midterm") #change to your own directory when running chunks in R
data_corpus_moviereviews <- data_corpus_moviereviews
dfm <- tokens(data_corpus_moviereviews, remove_punct = TRUE) |>
tokens_wordstem() |> #stemming so words of the same root are treated as the same word, providing better information for the model
tokens_remove(stopwords("english")) |> #removing stopwords so the model focuses on the most important words
dfm() #make a document-feature matrix
# shuffling to split into training and test set
set.seed(123) # for reproducibility
train<- sample(1:ndoc(dfm), 0.8*ndoc(dfm)) #split the data 80/20 into training and testing sets
test <- which(!1:ndoc(dfm) %in% train) #the rest of the data is the test set
docvars(data_corpus_moviereviews, "sentiment") <- docvars(data_corpus_moviereviews, "sentiment") |> relevel(ref="pos") #relevel the sentiment variable so that the positive class is the reference class
# training Naive Bayes model
nb <- textmodel_nb(dfm[train,], docvars(data_corpus_moviereviews, "sentiment")[train])
# predicting labels for test set
preds <- predict(nb, newdata = dfm[test,])
# actual labels for test set
acts <- docvars(data_corpus_moviereviews, "sentiment")[test]
# computing the confusion matrix
cm <- confusionMatrix(data=preds, reference = acts, positive="pos", mode="prec_recall")
cm$table |>
kbl(caption="Confusion matrix of the Naive Bayes Classifier", escape=F) |>
column_spec(1, bold = TRUE) |> kable_styling(full_width = F)
param <- t(nb$param) |> as.data.frame() |> mutate(posneg=pos/neg) |> arrange(desc(posneg)) #find out which words are most important for the classification from PwGc
View(param)
#dirname(rstudioapi::getSourceEditorContext()$path) #get file path
knitr::opts_chunk$set(
collapse = TRUE,
comment = "##",
message = FALSE,
warning = FALSE,
out.extra = "",
root.dir = "D:/OneDrive - London School of Economics/Desktop/lse assignments/MY459/summative-midterm"
)
rm(list=ls())
need <- c('tidyverse',"quanteda","quanteda.textmodels","kableExtra","caret","glmnet","glmnetUtils","parallel") # list packages needed
have <- need %in% rownames(installed.packages()) # checks packages you have
if(any(!have)) install.packages(need[!have]) # install missing packages
invisible(lapply(need, library, character.only=T))
setwd("D:/OneDrive - London School of Economics/Desktop/lse assignments/MY459/summative-midterm") #change to your own directory when running chunks in R
data_corpus_moviereviews <- data_corpus_moviereviews
dfm <- tokens(data_corpus_moviereviews, remove_punct = TRUE) |>
tokens_wordstem() |> #stemming so words of the same root are treated as the same word, providing better information for the model
tokens_remove(stopwords("english")) |> #removing stopwords so the model focuses on the most important words
dfm() #make a document-feature matrix
# shuffling to split into training and test set
set.seed(123) # for reproducibility
train<- sample(1:ndoc(dfm), 0.8*ndoc(dfm)) #split the data 80/20 into training and testing sets
test <- which(!1:ndoc(dfm) %in% train) #the rest of the data is the test set
docvars(data_corpus_moviereviews, "sentiment") <- docvars(data_corpus_moviereviews, "sentiment") |> relevel(ref="pos") #relevel the sentiment variable so that the positive class is the reference class
# training Naive Bayes model
nb <- textmodel_nb(dfm[train,], docvars(data_corpus_moviereviews, "sentiment")[train])
# predicting labels for test set
preds <- predict(nb, newdata = dfm[test,])
# actual labels for test set
acts <- docvars(data_corpus_moviereviews, "sentiment")[test]
# computing the confusion matrix
cm <- confusionMatrix(data=preds, reference = acts, positive="pos", mode="prec_recall")
cm$table |>
kbl(caption="Confusion matrix of the Naive Bayes Classifier", escape=F) |>
column_spec(1, bold = TRUE) |> kable_styling(full_width = F)
param <- t(nb$param) |> as.data.frame() |> mutate(posneg=pos/neg) |> arrange(desc(posneg)) #find out which words are most important for the classification from PwGc
#dfm_bi <- tokens(data_corpus_moviereviews, remove_punct = TRUE) |>
#  tokens_wordstem() |> #stemming so words of the same root are treated as the same word, providing better information for the model
#  tokens_remove(stopwords("english")) |> #removing stopwords so the model focuses on the most important words
#  tokens_ngrams(n = 1:2) |> #creating bigrams
#  dfm() |> #make a document-feature matrix
#  dfm_trim(min_termfreq = 2) #trim terms to prevent overfitting and speed up computation
#cross-validated regularised regression
#cl <- makeCluster(8) #create a cluster for parallel computing
#en_bi <- cva.glmnet(dfm_bi[train,], docvars(data_corpus_moviereviews, "sentiment")[train], family = "binomial", nfolds = 5, outerParallel = cl) #fit regularised logistic regressions on the training set with cross-validation
#stopCluster(cl) #stop the cluster
#find best cv.glmnet with best alphas
#find_model <- function(mods) {
#  loss <- sapply(mods$modlist,function(mod) mod$cvm[mod$lambda == mod$lambda.min])
#  best <- which.min(loss)
#  best_mod <- mods$modlist[[best]]
#  best_mod[['alpha']] <- mods$alpha[best]
#  return(best_mod)
#}
#en_bi <- find_model(en_bi) #find the best model
#save(en_bi, dfm_bi, file="en.RData") #save the model
load("en.RData") #load the model
pred_en_bi <- predict(en_bi, dfm_bi[test,], type="class") |> factor(levels=c("pos","neg")) #predict the test set
cm_en_bi <- confusionMatrix(data=pred_en_bi, reference = acts, positive="pos", mode="prec_recall") #compute the confusion matrix
cm_en_bi$table |>
kbl(caption="Confusion matrix of the Elastic Net Classifier wih bigrams and unigrams", escape=F) |>
column_spec(1, bold = TRUE) |> kable_styling(full_width = F) #print the confusion matrix
# may encounter path problem, run the line in console if necessary
#download.file('https://github.com/lse-my459/pset_data/raw/master/congress-tweets.csv', 'congress-tweets.csv')
# also download smaller sample of tweets
#download.file('https://github.com/lse-my459/pset_data/raw/master/congress-tweets-sample.csv', 'congress-tweets-sample.csv')
#download.file('https://github.com/lse-my459/pset_data/raw/master/candidate-tweets.csv', 'candidate-tweets.csv')
tweets <- read.csv('candidate-tweets.csv', stringsAsFactors=F) # similar issue may occur, run the line in console to solve the problem
#cong <- read.csv("congress-tweets.csv", stringsAsFactors=F)
# If later chunks of code are taking too long, uncomment the following line of code
# It is a 10% sample of the original data, so can be used for testing your answer
# In the final submission, please use the full data
# My advice would be to first knit a document with 10%, copy the html doc elsewhere as a backup,
# then perform a longer knit with the full data
#cong <- read.csv('congress-tweets-sample.csv', stringsAsFactors=F)
#trump <- paste(tweets$text[tweets$screen_name=="realDonaldTrump"], collapse="")
#clinton <- paste(tweets$text[tweets$screen_name=="HillaryClinton"], collapse="")
#cruz <- paste(tweets$text[tweets$screen_name=="tedcruz"], collapse="")
#sanders <- paste(tweets$text[tweets$screen_name=="BernieSanders"], collapse="")
# corpus - this line (and tokenisation later) might take a while, bear in mind for knitting and submission
# also see comment in cell above
#corp <- corpus(c(cong$text, trump, clinton, cruz, sanders))
#save(corp, file="wordscore.RData")
load("wordscore.RData")
corp
#docnames(corp) <- c(cong$screen_name, "Trump", "Clinton", "Cruz", "Sanders")
# reference scores and virgin scores
#refpoints <- c(cong$idealPoint, NA, NA, NA, NA)
#cdfm <- tokens(corp) |> dfm(remove_punct=TRUE) |> dfm_remove(c(stopwords("english"), "t.co", "https", "rt", "amp", "http", "t.c", "can")) #extremely time consuming
#bigrams are not used due to computational constraints
# trimming rare terms
#cdfm <- dfm_trim(cdfm, min_docfreq = 2)
#save(corp, cdfm, refpoints, file="wordscore.RData")
#load("wordscore.RData")
#no cross-validation because we are using wordscores
ws <- textmodel_wordscores(cdfm, refpoints, smooth=.5)
# predicted values
preds <- predict(ws, rescaling="lbg") #use LBG (2003) rescaling
load("ws_sm.Rdata") #load model trained on smaller sample for comparison
preds_sm <- predict(ws_sm, rescaling="lbg", newdata=cdfm)
# let's look at the most discriminant words
sw <- sort(coef(ws))
head(sw, n=20) #most left wing words
tail(sw, n=20) |> rev() #most right wing words
data.frame(preds = c(preds,preds_sm),
refpoints = rep(refpoints,2),
Sample = rep(c("Full","10%"), each = length(preds))) |> drop_na() |>
ggplot(aes(x=preds, y=refpoints, color=Sample)) +
geom_point() + theme_bw() +
geom_abline(intercept = 0, slope = 1, linetype = "dashed")  +
xlab("Wordscores estimates") +
ylab("Ideal points from 100 legislators")
tail(cdfm,4)
preds_ci <- predict(ws, newdata=tail(cdfm,4),rescaling="lbg", interval="confidence")[["fit"]] |> as.data.frame() |> mutate(Sample="Full") |>
rownames_to_column(var="name") #generate confidence intervals for the 4 candidates
preds_ci_sm <- predict(ws_sm, newdata=tail(cdfm,4), rescaling="lbg", interval="confidence")[["fit"]] |> as.data.frame() |> mutate(Sample="10%")|>
rownames_to_column(var="name")
preds_ci <- rbind(preds_ci, preds_ci_sm) #combine the two samples
preds_ci <- preds_ci |> mutate(name=fct_reorder(name,rep(fit[1:4],2) ))
# Set the width for dodging
dodge_width <- 0.5 #generated by ChatGPT
preds_ci$vjust <-  #generated by ChatGPT
ifelse(preds_ci$Sample == "Full", -0.5, 1.5)
ggplot(preds_ci, aes(x=fit, y=name, label=round(fit, 3), color=Sample)) + #generated by ChatGPT
geom_point(position = position_dodge(width = dodge_width)) +  # Add dodge to points
geom_text(size = 3, aes(vjust=vjust), position = position_dodge(width = dodge_width)) +  # Add dodge to text
geom_errorbarh(aes(xmin=lwr, xmax=upr), height=0, position = position_dodge(width = dodge_width)) +  # Add dodge to error bars
theme_bw() +
xlab("Wordscores estimates") +
ylab("Candidate") ### generated by copilot auto-completion
names(param) |> head(5)
rownames(param) |> head(5)
rownames(param) |> tail(5)
rownames(param) |> tail(5) |> rev()
